---
###############################################################################
# Consume flow messages from a pre-existing Kafka cluster containing
# protobuf-encoded flows in a topic. This topic can be generated by another
# pipeline, or by goflow itself.
- segment: kafkaconsumer
  config:
    server: kafka01.example.com:9093
    topic: flow-messages-plain
    group: enricher-group-1
    user: enricher
    pass: $KAFKA_SASL_PASS

###############################################################################
# Filter flows for relevancy before proceeding to enrich them.
- segment: flowfilter
  config:
    filter: address 0.0.0.0/0 or address ::/0  # optionally filter for relevancy here

###############################################################################
# This tags all flows with the information whether their source or destination
# address is the remote address, thus also allowing the inference which one
# will be the local one.
#
# The different options are:
# 1. "border", which assumes all flows are collected on the network border
#    interfaces, thus making ingress flows into that interface have a remote
#    source address, and vice versa.
# 2. "user", which assumes all flows are collected on a user interface, thus
#    making ingress flows have a remote destination address. In this case,
#    remote from the user's point of view (not the network operators pov), as
#    the flow could have had another user as its destination.
# 3. "mixed", as we can't make any assumptions in this case, all remote address
#    information is cleared from all flows in this mode.
- segment: remoteaddress
  config:
    flowsrc: "border"

###############################################################################
# Add a customer id field to all flows by CIDR match. As this works on local
# addresses only, we need to run after the 'remoteaddress' segment.
- segment: addcid
  config:
    filename: customer_subnets.csv
    dropunmatched: 1  # limit to flows belonging to customers

###############################################################################
# Add a geolocation field to all flows. As this works on remote addresses (the
# assumption being that your own addresses are national), we need to run after
# the 'remoteaddress' segment.
- segment: geolocation
  config:
    filename: GeoLite2-Country-Test.mmdb

###############################################################################
# Add human-readable interface data for both interfaces, this specifically
# means the name, description and speed.
# The first (few) flows passing this segment will not have this information
# added, as routers are queried asynchronously. The cache will last an hour.
- segment: snmpinterface
  config:
    community: public
    # optionally supply a regex to strip down interface descriptions
    #regex: "^[a-z0-9]: (.*)$"  # strips prefix, keeps the leftmost group

###############################################################################
# Normalize Bytes and Packets using the in-flow SamplingRate or the provided
# fallback. Also sets the Normalized field to true. Does not do anything if
# neither sampling rate or fallback is non-zero.
- segment: normalize
  config:
    fallback: 32

###############################################################################
# Produce flow messages back to a Kafka topic. This can be a different one than
# above.
- segment: kafkaproducer
  config:
    server: kafka01.example.com:9093
    topic: flow-messages-enriched
    user: enricher
    pass: $KAFKA_SASL_PASS
